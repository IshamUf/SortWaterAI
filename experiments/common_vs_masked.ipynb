{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T02:00:56.124704Z",
     "start_time": "2025-05-20T02:00:56.113402Z"
    }
   },
   "source": [
    "# Окружение и обёртка из Masked DQN, но без изменений\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "class WaterSortEnvFixed(gym.Env):\n",
    "    \"\"\"\n",
    "    Окружение Water Sort Puzzle с фиксированным числом пробирок и слоёв.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_tubes=4, max_layers=4, num_empty=1, num_colors=3, max_steps=300):\n",
    "        super().__init__()\n",
    "        self.num_tubes = num_tubes\n",
    "        self.max_layers = max_layers\n",
    "        self.num_empty = num_empty\n",
    "        self.num_colors = num_colors\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # MultiDiscrete([from, to])\n",
    "        self.action_space = spaces.MultiDiscrete([num_tubes, num_tubes])\n",
    "        # Box state: (N, K)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-1, high=num_colors - 1,\n",
    "            shape=(num_tubes, max_layers), dtype=int\n",
    "        )\n",
    "\n",
    "        self.state = None\n",
    "        self.steps = 0\n",
    "        self.prev_action = None\n",
    "        self.recent_states = deque(maxlen=10)\n",
    "\n",
    "    def reset(self, seed=None, options=None, previous=False):\n",
    "        super().reset(seed=seed)\n",
    "        self.recent_states.clear()\n",
    "\n",
    "        if previous and hasattr(self, \"prev_state\"):\n",
    "            self.state = self.prev_state.copy()\n",
    "        else:\n",
    "            # Генерация нового уровня\n",
    "            while True:\n",
    "                self.state = np.full((self.num_tubes, self.max_layers), -1, dtype=int)\n",
    "                filled = self.num_tubes - self.num_empty\n",
    "                slots = filled * self.max_layers\n",
    "                counts = slots // self.num_colors\n",
    "                colors = np.repeat(np.arange(self.num_colors), counts)\n",
    "                self.np_random.shuffle(colors)\n",
    "                idx = 0\n",
    "                for t in range(filled):\n",
    "                    for l in range(self.max_layers):\n",
    "                        self.state[t, l] = colors[idx];\n",
    "                        idx += 1\n",
    "                if not self._is_solved():\n",
    "                    break\n",
    "\n",
    "        self.steps = 0\n",
    "        obs = self._get_obs()\n",
    "        self.prev_state = obs.copy()\n",
    "        self.prev_action = None\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        frm, to = action\n",
    "        reward = -1\n",
    "        info = {}\n",
    "\n",
    "        # проверка валидности\n",
    "        if frm == to or frm < 0 or to < 0 or frm >= self.num_tubes or to >= self.num_tubes:\n",
    "            reward -= 0.05\n",
    "        elif self.prev_action == (frm, to):\n",
    "            reward -= 0.1\n",
    "        elif self._can_pour(frm, to):\n",
    "            before = self._count_sorted()\n",
    "            self._pour(frm, to)\n",
    "            after = self._count_sorted()\n",
    "            reward += 1 if after > before else 0\n",
    "        else:\n",
    "            reward -= 0.05\n",
    "\n",
    "        self.prev_action = (frm, to)\n",
    "        self.steps += 1\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        done = self._is_solved()\n",
    "        no_moves = len(self.fast_get_valid_actions(obs.flatten())) == 0\n",
    "        truncated = self.steps >= self.max_steps or no_moves\n",
    "\n",
    "        # штраф за повтор состояния\n",
    "        tup = tuple(obs.flatten())\n",
    "        if tup in self.recent_states:\n",
    "            reward -= 3.0\n",
    "        self.recent_states.append(tup)\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self.state.copy()\n",
    "\n",
    "    def _is_solved(self):\n",
    "        for tube in self.state:\n",
    "            if np.all(tube == -1): continue\n",
    "            if np.any(tube == -1): return False\n",
    "            if len(set(tube)) != 1: return False\n",
    "        return True\n",
    "\n",
    "    def _count_sorted(self):\n",
    "        cnt = 0\n",
    "        for tube in self.state:\n",
    "            if np.any(tube == -1): continue\n",
    "            if len(set(tube)) == 1: cnt += 1\n",
    "        return cnt\n",
    "\n",
    "    def _find_top(self, tube):\n",
    "        for i in range(self.max_layers):\n",
    "            if self.state[tube, i] != -1: return i\n",
    "        return -1\n",
    "\n",
    "    def _can_pour(self, frm, to):\n",
    "        f = self._find_top(frm)\n",
    "        if f < 0: return False\n",
    "        t = self._find_top(to)\n",
    "        if t == 0: return False\n",
    "        c = self.state[frm, f]\n",
    "        if t < 0: return True\n",
    "        return c == self.state[to, t]\n",
    "\n",
    "    def _pour(self, frm, to):\n",
    "        f = self._find_top(frm)\n",
    "        color = self.state[frm, f]\n",
    "        cnt = 1\n",
    "        i = f + 1\n",
    "        while i < self.max_layers and self.state[frm, i] == color:\n",
    "            cnt += 1;\n",
    "            i += 1\n",
    "        t = self._find_top(to)\n",
    "        dest = self.max_layers - 1 if t < 0 else t - 1\n",
    "        while cnt > 0 and dest >= 0:\n",
    "            if self.state[to, dest] == -1:\n",
    "                self.state[to, dest] = color\n",
    "                self.state[frm, f] = -1\n",
    "                f += 1;\n",
    "                dest -= 1;\n",
    "                cnt -= 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    def fast_get_valid_actions(self, flat_obs, *, ignore_prev=False):\n",
    "        N, K = self.num_tubes, self.max_layers\n",
    "        obs = flat_obs.reshape(N, K)\n",
    "        empty = (obs == -1).all(axis=1)\n",
    "        tops = np.where(empty, K, (obs != -1).argmax(axis=1))\n",
    "        free = K - (K - tops)\n",
    "        topcol = np.full(N, -2, dtype=int)\n",
    "        mask_fill = ~empty\n",
    "        topcol[mask_fill] = obs[mask_fill, tops[mask_fill]]\n",
    "        can_from = ~empty\n",
    "        can_to = free > 0\n",
    "        same_or_empty = (topcol[:, None] == topcol[None, :]) | empty[None, :]\n",
    "        mask = can_from[:, None] & can_to[None, :] & same_or_empty\n",
    "        np.fill_diagonal(mask, False)\n",
    "        if not ignore_prev and self.prev_action is not None:\n",
    "            idx = self.prev_action[0] * N + self.prev_action[1]\n",
    "            mask.flat[idx] = False\n",
    "        return np.flatnonzero(mask.ravel()).tolist()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T01:54:25.836208Z",
     "start_time": "2025-05-20T01:54:25.825981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DiscreteActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Преобразование MultiDiscrete→Discrete и flatten(obs).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        N = env.num_tubes\n",
    "        self.env = env\n",
    "        self.action_space = spaces.Discrete(N * N)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-1, high=env.num_colors - 1,\n",
    "            shape=(N * env.max_layers,), dtype=int\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs.flatten(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        N = self.env.num_tubes\n",
    "        frm, to = divmod(action, N)\n",
    "        obs, reward, done, trunc, info = self.env.step((frm, to))\n",
    "        return obs.flatten(), reward, done, trunc, info\n",
    "\n",
    "    def fast_get_valid_actions(self, flat_obs, *, ignore_prev=False):\n",
    "        return self.env.fast_get_valid_actions(flat_obs, ignore_prev=ignore_prev)\n",
    "\n",
    "\n",
    "# Обычный DQN-агент (Vanilla), без маскирования\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class VanillaDQNAgent(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla DQN + target-net. ε-жадный по всему action_space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, net_arch=[256, 256], lr=1e-4, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Q-сеть\n",
    "        layers = []\n",
    "        inp = state_dim\n",
    "        for h in net_arch:\n",
    "            layers += [nn.Linear(inp, h), nn.ReLU()]\n",
    "            inp = h\n",
    "        layers.append(nn.Linear(inp, action_dim))\n",
    "        self.q_net = nn.Sequential(*layers).to(device)\n",
    "\n",
    "        # target-сеть\n",
    "        self.q_net_target = copy.deepcopy(self.q_net)\n",
    "        for p in self.q_net_target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.q_net(x)\n",
    "\n",
    "    def predict_qvalues(self, obs: np.ndarray) -> np.ndarray:\n",
    "        t = torch.FloatTensor(obs).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q = self.q_net(t).cpu().numpy()\n",
    "        return q\n",
    "\n",
    "    def sample_actions(self, obs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        ε-жадный по всему action_dim.\n",
    "        \"\"\"\n",
    "        q = self.predict_qvalues(obs)  # shape (B,A)\n",
    "        B, A = q.shape\n",
    "        acts = np.empty(B, dtype=int)\n",
    "        for i in range(B):\n",
    "            if random.random() < self.epsilon:\n",
    "                acts[i] = random.randrange(A)\n",
    "            else:\n",
    "                acts[i] = int(np.argmax(q[i]))\n",
    "        return acts\n",
    "\n",
    "    def update_target(self):\n",
    "        self.q_net_target.load_state_dict(self.q_net.state_dict())\n"
   ],
   "id": "5ca7f24db74784fc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T01:54:27.515429Z",
     "start_time": "2025-05-20T01:54:27.287479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10**5):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=bool)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def compute_vanilla_dqn_loss(agent: VanillaDQNAgent,\n",
    "                             env,            # нужен только для сигнатуры\n",
    "                             batch,\n",
    "                             gamma=0.99,\n",
    "                             device='cpu'):\n",
    "    \"\"\"\n",
    "    MSE-loss для Vanilla DQN:\n",
    "      target = r + γ * max_a' Q_target(s',a') * (1 - done)\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "    B = len(states)\n",
    "\n",
    "    states_t      = torch.FloatTensor(states).to(device)\n",
    "    actions_t     = torch.LongTensor(actions).to(device)\n",
    "    rewards_t     = torch.FloatTensor(rewards).to(device)\n",
    "    next_states_t = torch.FloatTensor(next_states).to(device)\n",
    "    dones_t       = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    # Q(s,a)\n",
    "    q_values = agent.q_net(states_t)                   # (B, A)\n",
    "    q_taken  = q_values[range(B), actions_t]           # (B,)\n",
    "\n",
    "    # target via target-network\n",
    "    with torch.no_grad():\n",
    "        q_next_target, _ = agent.q_net_target(next_states_t).max(dim=1)  # (B,)\n",
    "        target = rewards_t + gamma * q_next_target * (~dones_t)\n",
    "\n",
    "    loss = torch.nn.functional.mse_loss(q_taken, target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def evaluate_by_mean(env_eval,\n",
    "                     agent,\n",
    "                     n_games: int,\n",
    "                     bfs_timeout: int = 100_000):\n",
    "    \"\"\"\n",
    "    Возвращает ту же группу метрик, что и в masked evaluate_by_mean,\n",
    "    но агент выбирает actions через ε-greedy без маски.\n",
    "    \"\"\"\n",
    "    rewards_all, steps_all    = [], []\n",
    "    rewards_clean, steps_clean= [], []\n",
    "    steps_math                = []\n",
    "\n",
    "    n_win_games = n_step_limit = n_no_moves = 0\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        obs0, _ = env_eval.reset(previous=False)\n",
    "        obs     = obs0.copy()\n",
    "        total_r = 0.0\n",
    "        agent_steps = 0\n",
    "\n",
    "        # игра агента\n",
    "        while True:\n",
    "            old_eps = agent.epsilon\n",
    "            agent.epsilon = 0.0\n",
    "            action = agent.sample_actions(obs[None])[0]\n",
    "            agent.epsilon = old_eps\n",
    "\n",
    "            obs, rew, terminated, truncated, info = env_eval.step(action)\n",
    "            total_r   += rew\n",
    "            agent_steps += 1\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards_all.append(total_r)\n",
    "        steps_all.append(agent_steps)\n",
    "\n",
    "        clean_ep = not info.get(\"step_limit_reached\", False) \\\n",
    "                   and not info.get(\"no_valid_moves\", False)\n",
    "\n",
    "        if clean_ep:\n",
    "            rewards_clean.append(total_r)\n",
    "            steps_clean.append(agent_steps)\n",
    "            n_win_games += 1\n",
    "\n",
    "            if terminated:\n",
    "                path_opt, len_opt, *_ = solve_optimal_path(\n",
    "                    env_eval, obs0, max_expansions=bfs_timeout)\n",
    "                if len_opt is not None:\n",
    "                    steps_math.append(len_opt)\n",
    "        else:\n",
    "            # уровень не «чисто» решён — проверяем решаемость\n",
    "            path_opt, len_opt, *_ = solve_optimal_path(\n",
    "                env_eval, obs0, max_expansions=bfs_timeout)\n",
    "            if len_opt is not None:\n",
    "                n_step_limit += 1\n",
    "            else:\n",
    "                n_no_moves += 1\n",
    "\n",
    "    # агрегаты\n",
    "    mean_reward_all   = float(np.mean(rewards_all))\n",
    "    mean_steps_all    = float(np.mean(steps_all))\n",
    "    mean_reward_clean = float(np.mean(rewards_clean)) if rewards_clean else float(\"nan\")\n",
    "    mean_steps_clean  = float(np.mean(steps_clean)) if steps_clean else float(\"nan\")\n",
    "    mean_steps_math   = float(np.mean(steps_math)) if steps_math else float(\"nan\")\n",
    "    delta_steps       = mean_steps_clean - mean_steps_math if not np.isnan(mean_steps_math) else float(\"nan\")\n",
    "\n",
    "    return (\n",
    "        mean_reward_all,   mean_steps_all,\n",
    "        mean_reward_clean, mean_steps_clean,\n",
    "        mean_steps_math,   delta_steps,\n",
    "        n_win_games,       n_step_limit, n_no_moves\n",
    "    )\n",
    "\n",
    "\n",
    "def draw_graphs(r_all, s_all,\n",
    "                r_clean, s_clean,\n",
    "                s_math, delt,\n",
    "                n_win, n_limit, n_nomove,\n",
    "                eval_period, total_timesteps):\n",
    "    \"\"\"\n",
    "    Тот же 5-панельный дашборд, что и у вас в masked DQN.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    x = np.arange(1, len(r_all) + 1) * eval_period\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 15))\n",
    "    gs  = fig.add_gridspec(3, 2,\n",
    "                           height_ratios=[1, 0.9, 0.6],\n",
    "                           width_ratios =[1, 0.8])\n",
    "\n",
    "    # ① Mean reward\n",
    "    ax0 = fig.add_subplot(gs[0, 0])\n",
    "    ax0.set_title(\"Mean reward\")\n",
    "    ax0.plot(x, r_all,   label=\"all\")\n",
    "    ax0.plot(x, r_clean, label=\"clean\")\n",
    "    ax0.set_ylabel(\"R\"); ax0.grid(); ax0.legend()\n",
    "\n",
    "    # ② Mean steps (+ ideal)\n",
    "    ax1 = fig.add_subplot(gs[0, 1])\n",
    "    ax1.set_title(\"Mean steps\")\n",
    "    ax1.plot(x, s_all,   label=\"all\")\n",
    "    ax1.plot(x, s_clean, label=\"clean\")\n",
    "    ax1.plot(x, s_math,  label=\"ideal\")\n",
    "    ax1.set_ylabel(\"steps\"); ax1.grid(); ax1.legend()\n",
    "\n",
    "    # ③ Episode counters\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax2.set_title(\"Episode counters / evaluation\")\n",
    "    ax2.plot(x, n_win,    label=\"wins\")\n",
    "    ax2.plot(x, n_limit,  label=\"step-limit\")\n",
    "    ax2.plot(x, n_nomove, label=\"no-moves\")\n",
    "    ax2.set_xlabel(f\"env steps (×{eval_period})\")\n",
    "    ax2.set_ylabel(\"episodes / 100 runs\")\n",
    "    ax2.grid(); ax2.legend()\n",
    "\n",
    "    # ④ Summary table\n",
    "    ax3 = fig.add_subplot(gs[1, 1]);  ax3.axis(\"off\")\n",
    "    last = lambda arr: (arr[-1] if arr else float(\"nan\"))\n",
    "    table_data = [\n",
    "        [\"reward all\",   f\"{last(r_all):.2f}\"],\n",
    "        [\"reward clean\", f\"{last(r_clean):.2f}\"],\n",
    "        [\"steps  all\",   f\"{last(s_all):.1f}\"],\n",
    "        [\"steps  clean\", f\"{last(s_clean):.1f}\"],\n",
    "        [\"steps  math\",  f\"{last(s_math):.1f}\"],\n",
    "        [\"Δ clean-math\", f\"{last(delt):+.1f}\"],\n",
    "        [\"wins\",         int(last(n_win))],\n",
    "        [\"step-limit\",   int(last(n_limit))],\n",
    "        [\"no-moves\",     int(last(n_nomove))],\n",
    "    ]\n",
    "    tbl = ax3.table(cellText=table_data,\n",
    "                    colLabels=[\"Metric\", \"Last value\"],\n",
    "                    loc=\"center\", cellLoc=\"center\")\n",
    "    tbl.scale(1, 2.1); tbl.auto_set_font_size(False); tbl.set_fontsize(12)\n",
    "\n",
    "    # ⑤ Δ-steps plot\n",
    "    ax4 = fig.add_subplot(gs[2, 0])\n",
    "    ax_dummy = fig.add_subplot(gs[2, 1])\n",
    "    ax_dummy.axis(\"off\")\n",
    "    ax4.set_title(\"Δ-steps (clean − math)\")\n",
    "    ax4.plot(x, delt, label=\"Δ steps\", color=\"tab:red\")\n",
    "    ax4.set_xlabel(\"environment steps\"); ax4.set_ylabel(\"Δ\"); ax4.grid(); ax4.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n"
   ],
   "id": "446f19f1815abf03",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T01:59:24.323853Z",
     "start_time": "2025-05-20T01:59:24.317240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def solve_optimal_path(wrapped_env,\n",
    "                       flat_obs: np.ndarray,\n",
    "                       max_expansions: int = 100_000):\n",
    "    \"\"\"\n",
    "    BFS-поиск кратчайшего решения для текущего состояния.\n",
    "\n",
    "    Возврат:\n",
    "        path        – список действий (формат Discrete, как в env.action_space)\n",
    "        length      – длина решения (None, если не найдено)\n",
    "        expansions  – сколько состояний было развёрнуто\n",
    "        t_seconds   – время работы поиска\n",
    "    \"\"\"\n",
    "    # параметры головоломки\n",
    "    env = wrapped_env.env\n",
    "    N, K = env.num_tubes, env.max_layers\n",
    "\n",
    "    start_tuple = tuple(flat_obs)                 # неизменяемое представление\n",
    "\n",
    "    # ── вспомогательные чистые функции на numpy-матрицах ──────────────────────\n",
    "    def is_solved(mat: np.ndarray) -> bool:\n",
    "        for t in range(N):\n",
    "            tube = mat[t]\n",
    "            if np.all(tube == -1):                     # полностью пустая\n",
    "                continue\n",
    "            if np.any(tube == -1) or len(set(tube)) != 1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def find_top(mat, tube):\n",
    "        for i in range(K):\n",
    "            if mat[tube, i] != -1:\n",
    "                return i\n",
    "        return -1\n",
    "\n",
    "    def can_pour(mat, fr, to):\n",
    "        if fr == to:\n",
    "            return False\n",
    "        fr_top = find_top(mat, fr)\n",
    "        if fr_top == -1:\n",
    "            return False\n",
    "        to_top = find_top(mat, to)\n",
    "        if to_top == 0:                              # приёмник полон\n",
    "            return False\n",
    "        from_color = mat[fr, fr_top]\n",
    "        return (to_top == -1) or (mat[to, to_top] == from_color)\n",
    "\n",
    "    def pour(mat, fr, to):\n",
    "        \"\"\"возвращает НОВУЮ матрицу после переливания fr→to\"\"\"\n",
    "        mat = mat.copy()\n",
    "        fr_top = find_top(mat, fr)\n",
    "        color  = mat[fr, fr_top]\n",
    "\n",
    "        # сколько одинаковых цветных слоёв сверху?\n",
    "        count = 1\n",
    "        idx = fr_top + 1\n",
    "        while idx < K and mat[fr, idx] == color:\n",
    "            count += 1; idx += 1\n",
    "\n",
    "        to_top = find_top(mat, to)\n",
    "        to_idx = K - 1 if to_top == -1 else to_top - 1\n",
    "\n",
    "        while count and to_idx >= 0 and mat[to, to_idx] == -1:\n",
    "            mat[to, to_idx] = color\n",
    "            mat[fr, fr_top] = -1\n",
    "            fr_top += 1\n",
    "            to_idx -= 1\n",
    "            count -= 1\n",
    "        return mat\n",
    "\n",
    "    # ── сам BFS ───────────────────────────────────────────────────────────────\n",
    "    t0 = time.time()\n",
    "    q = deque([(start_tuple, [])])\n",
    "    visited = {start_tuple}\n",
    "    expansions = 0\n",
    "\n",
    "    while q and expansions < max_expansions:\n",
    "        state_tuple, path = q.popleft()\n",
    "        state_mat = np.array(state_tuple).reshape(N, K)\n",
    "\n",
    "        if is_solved(state_mat):\n",
    "            return path, len(path), expansions, time.time() - t0\n",
    "\n",
    "        for a in wrapped_env.fast_get_valid_actions(np.array(state_tuple),\n",
    "                                                    ignore_prev=True):\n",
    "            fr, to = divmod(a, N)\n",
    "            if not can_pour(state_mat, fr, to):\n",
    "                continue\n",
    "            nxt = pour(state_mat, fr, to)\n",
    "            nxt_tuple = tuple(nxt.flatten())\n",
    "            if nxt_tuple not in visited:\n",
    "                visited.add(nxt_tuple)\n",
    "                q.append((nxt_tuple, path + [a]))\n",
    "\n",
    "        expansions += 1\n",
    "\n",
    "    # не нашли решение\n",
    "    return [], None, expansions, time.time() - t0"
   ],
   "id": "70869fff77e941b2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T01:54:28.522126Z",
     "start_time": "2025-05-20T01:54:28.510857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_vanilla_dqn(\n",
    "    env,\n",
    "    env_eval,\n",
    "    agent,\n",
    "    total_timesteps: int = 200_000,\n",
    "    device: str = 'cpu',\n",
    "    evaluate_interval: int = 10_000\n",
    "):\n",
    "    # Параметры тренинга\n",
    "    buffer_size = 10**5\n",
    "    batch_size = 64\n",
    "    learning_starts = 10_000\n",
    "    train_freq = 4\n",
    "    target_update_interval = 10_000\n",
    "    gamma = 0.99\n",
    "\n",
    "    exploration_fraction = 0.5\n",
    "    exploration_final_eps = 0.1\n",
    "    max_grad_norm = 10.0\n",
    "\n",
    "    replay_buffer = ReplayBuffer(capacity=buffer_size)\n",
    "\n",
    "    def linear_epsilon(step):\n",
    "        frac = min(step / (exploration_fraction * total_timesteps), 1.0)\n",
    "        return 1.0 + frac * (exploration_final_eps - 1.0)\n",
    "\n",
    "    # ——— Начальное заполнение буфера —————————————\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(learning_starts):\n",
    "        # ε-greedy без маски\n",
    "        if random.random() < 1.0:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.sample_actions(obs[None])[0]\n",
    "\n",
    "        next_obs, reward, done, truncated, _ = env.step(action)\n",
    "        replay_buffer.add(obs, action, reward, next_obs, done or truncated)\n",
    "        obs = next_obs\n",
    "        if done or truncated:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    # ——— Логгеры —————————————————————————————————\n",
    "    rewards_history = []\n",
    "    steps_history = []\n",
    "\n",
    "    # Для evaluate_by_mean\n",
    "    ev_r_all, ev_s_all = [], []\n",
    "    ev_r_clean, ev_s_clean = [], []\n",
    "    ev_s_math, ev_delt = [], []\n",
    "    ev_n_win, ev_n_limit, ev_n_nomove = [], [], []\n",
    "\n",
    "    # Старт эпизода\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0.0\n",
    "    ep_steps = 0\n",
    "    global_step = 0\n",
    "    figs = None\n",
    "\n",
    "    for step in trange(total_timesteps):\n",
    "        # 1) обновляем ε\n",
    "        agent.epsilon = linear_epsilon(step)\n",
    "\n",
    "        # 2) выбор действия ε-greedy (без маски)\n",
    "        action = agent.sample_actions(obs[None])[0]\n",
    "\n",
    "        # 3) шаг в среде\n",
    "        next_obs, reward, done, truncated, _ = env.step(action)\n",
    "        replay_buffer.add(obs, action, reward, next_obs, done or truncated)\n",
    "        obs = next_obs\n",
    "\n",
    "        ep_reward += reward\n",
    "        ep_steps += 1\n",
    "        global_step += 1\n",
    "\n",
    "        if done or truncated:\n",
    "            rewards_history.append(ep_reward)\n",
    "            steps_history.append(ep_steps)\n",
    "            ep_reward = 0.0\n",
    "            ep_steps = 0\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "        # 4) обучение\n",
    "        if global_step % train_freq == 0 and len(replay_buffer) >= batch_size:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            loss = compute_vanilla_dqn_loss(agent, env, batch, gamma, device)\n",
    "            agent.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "            agent.optimizer.step()\n",
    "\n",
    "        # 5) обновление target\n",
    "        if global_step % target_update_interval == 0:\n",
    "            agent.update_target()\n",
    "\n",
    "        # 6) оценка\n",
    "        if global_step % evaluate_interval == 0:\n",
    "            (mean_r_all,  mean_s_all,\n",
    "             mean_r_clean, mean_s_clean,\n",
    "             mean_s_math, delta_steps,\n",
    "             n_win, n_limit, n_no_moves) = evaluate_by_mean(\n",
    "                env_eval, agent, n_games=100, bfs_timeout=100_000\n",
    "            )\n",
    "\n",
    "            ev_r_all  .append(mean_r_all)\n",
    "            ev_s_all  .append(mean_s_all)\n",
    "            ev_r_clean.append(mean_r_clean)\n",
    "            ev_s_clean.append(mean_s_clean)\n",
    "            ev_s_math .append(mean_s_math)\n",
    "            ev_delt   .append(delta_steps)\n",
    "            ev_n_win  .append(n_win)\n",
    "            ev_n_limit.append(n_limit)\n",
    "            ev_n_nomove.append(n_no_moves)\n",
    "\n",
    "            figs = draw_graphs(\n",
    "                ev_r_all, ev_s_all,\n",
    "                ev_r_clean, ev_s_clean,\n",
    "                ev_s_math, ev_delt,\n",
    "                ev_n_win, ev_n_limit, ev_n_nomove,\n",
    "                evaluate_interval, total_timesteps\n",
    "            )\n",
    "\n",
    "    return agent, rewards_history, steps_history, figs\n"
   ],
   "id": "2304b8bb44db18f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Список моделей (N_K_L)\n",
    "filenames = [\n",
    "\n",
    "    \"5_1_4\", \"7_2_4\", \"7_2_5\",\n",
    "\n",
    "]\n",
    "\n",
    "total_timesteps     = 500_000\n",
    "evaluate_interval   = total_timesteps // 10\n",
    "\n",
    "for name in filenames:\n",
    "    n, k, l = map(int, name.split('_'))\n",
    "\n",
    "    # фиксация сидов для повторимости\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # создаём среду и обёртку\n",
    "    max_steps_per_game = 100\n",
    "    base_env      = WaterSortEnvFixed(n, l, num_empty=k, num_colors=n-k, max_steps=max_steps_per_game)\n",
    "    env           = DiscreteActionWrapper(base_env)\n",
    "    base_env_eval = WaterSortEnvFixed(n, l, num_empty=k, num_colors=n-k, max_steps=max_steps_per_game)\n",
    "    env_eval      = DiscreteActionWrapper(base_env_eval)\n",
    "\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available()\n",
    "        else \"mps\"   if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"[{name}] Using device: {device}\")\n",
    "\n",
    "    # создаём ванильного агента\n",
    "    lr      = 1e-4\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.n\n",
    "\n",
    "    agent = VanillaDQNAgent(\n",
    "        state_dim = obs_dim,\n",
    "        action_dim= act_dim,\n",
    "        net_arch  = [(n*(n-1))*25, (n*(n-1))*10],\n",
    "        lr        = lr,\n",
    "        device    = device\n",
    "    )\n",
    "\n",
    "    # запускам обучение\n",
    "    agent, rewards_history, steps_history, fig = train_vanilla_dqn(\n",
    "        env,\n",
    "        env_eval,\n",
    "        agent,\n",
    "        total_timesteps   = total_timesteps,\n",
    "        device            = device,\n",
    "        evaluate_interval = evaluate_interval\n",
    "    )\n",
    "\n",
    "    # сохраняем веса и последний график\n",
    "    model_path = f\"relevante/unmasked/dqn_{n}_{k}_{l}_{total_timesteps}.pth\"\n",
    "    torch.save(agent.state_dict(), model_path)\n",
    "\n",
    "    if fig is not None:\n",
    "        out_png = f\"graphs/unmasked/dqn_{n}_{k}_{l}_{total_timesteps}.png\"\n",
    "        fig.savefig(out_png)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"[{name}] done, model → {model_path}\\n\")\n"
   ],
   "id": "2423efc39b560da0",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [10:00<00:00, 832.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7_2_5] done, model → relevante/unmasked/dqn_7_2_5_500000.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "496f9c034550c1a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
